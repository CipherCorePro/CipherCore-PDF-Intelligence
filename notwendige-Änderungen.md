---

## ‚úÖ **Was ge√§ndert werden musste ‚Äì Klar dokumentiert**

---

### 1. **Fehlermanagement f√ºr spaCy-Modell eingef√ºhrt**

**Warum?**
Im urspr√ºnglichen Code wurde `nlp = spacy.load("de_core_news_sm")` direkt aufgerufen ‚Äì das f√ºhrt zu einem **harten Fehler**, wenn das Modell nicht installiert ist.

**Nachbearbeitet:**
```python
try:
    nlp = spacy.load("de_core_news_sm")  # ‚Üê ge√§ndert
except OSError:
    print("Fehler: Das spaCy-Modell 'de_core_news_sm' wurde nicht gefunden.")  # ‚Üê ge√§ndert
    print("Stelle sicher, dass du es mit 'python -m spacy download de_core_news_sm' heruntergeladen hast.")  # ‚Üê ge√§ndert
    exit()  # ‚Üê ge√§ndert
```
‚û°Ô∏è **Zweck:** Freundlichere Fehlerbehandlung, klare Anleitung f√ºr den User, Exit ohne Crash.

---

### 2. **Robuste Dateipfad-Eingabe via `input()`**

**Warum?**
Die Agentenversion hatte einen statischen Pfad `"pfad/zur/pdf_aus_vorheriger_iteration.pdf"` ‚Äì das ist nicht interaktiv und f√ºhrt ins Leere.

**Nachbearbeitet:**
```python
pdf_path = input("Bitte gib den Pfad zur PDF-Datei ein: ")  # ‚Üê ge√§ndert
```
‚û°Ô∏è **Zweck:** Benutzerfreundliche Eingabe bei Laufzeit ‚Äì deutlich flexibler.

---

### 3. **Pr√ºfung der R√ºckgabewerte im Hauptprogramm**

**Warum?**
Die Agentenversion pr√ºfte `if pdf_keywords_tfidf and sortierte_pdf_entities_haeufigkeit and pdf_saetze ...`, was problematisch ist, wenn `None` zur√ºckkommt.

**Nachbearbeitet:**
```python
if pdf_keywords or sorted_pdf_entities or pdf_sentences or filtered_pdf_entities:  # ‚Üê ge√§ndert
```
‚û°Ô∏è **Zweck:** Sichere Pr√ºfung auch bei leerer Liste ‚Äì verhindert unerwartete Fehler.

---

### 4. **Entit√§tenz√§hlung angepasst (Tuple unpacking)**

**Warum?**
Im Agentencode wurde `Counter` auf `(ent.text, ent.label_)` angewendet. Der R√ºckgabewert `most_common()` ergibt `[((Text, Label), Count)]`. Im Code wurde das korrekt gehandhabt ‚Äì das ist erhalten geblieben und war **schon gut**.

‚û°Ô∏è **Fazit:** Diese Struktur hast du beibehalten und korrekt verarbeitet ‚Äì ‚úÖ

---

### 5. **Erweiterte Beispiels√§tze und Fragen**

Du hast im Hauptprogramm die Fragen deutlich **n√§her am Realfall** formuliert:
```python
"Welche Organisationen werden in dem Dokument erw√§hnt?",  # ‚Üê ge√§ndert
"Wer sind die Autoren in diesem Dokument?",  # ‚Üê ge√§ndert
"Was sind die wichtigsten Ergebnisse?",  # ‚Üê ge√§ndert
```
‚û°Ô∏è **Zweck:** Die Testfragen pr√ºfen sowohl die Entity-Aware-Logik (ORG/PER) als auch den Keyword-Fallback.

---

### 6. **Syntaxbereinigung und Typannotationen**

Du hast z.‚ÄØB. die R√ºckgabe von `process_pdf` wie folgt klar deklariert:
```python
def process_pdf(pdf_path: str) -> tuple[list, list, list, list]:  # ‚Üê ge√§ndert
```
‚û°Ô∏è **Zweck:** H√∂here Lesbarkeit, Python 3.9+ Feature ‚Äì **modern & sauber**!

---

## üîÅ Vorher-Nachher Vergleich (Diff-Tabelle)

| üîÑ Stelle | üõ†Ô∏è Vorher | ‚úÖ Nachher |
|----------|-----------|-----------|
| spaCy-Modell | `nlp = spacy.load(...)` | `try: nlp = ... except OSError: ...` |
| PDF-Pfad | statischer String | `input("Pfad eingeben")` |
| R√ºckgabepr√ºfung | `if pdf_keywords_tfidf and ...` | `if pdf_keywords or ...` |
| Fragen | Dummy-Fragen | Realit√§tsnahe Fragen (ORG/PER/Testfall) |
| R√ºckgabetyp | nicht angegeben | `-> tuple[list, ...]` |

---

## üîç Fazit:

Deine Nachbearbeitung hat den Code:

- robuster (Fehlerbehandlung),
- interaktiver (Pfad-Eingabe),
- praxisn√§her (Testfragen),
- klarer strukturiert (Funktions√ºberschriften, Typisierung)
- und produktionstauglich gemacht. ‚úÖ

---

---

## ‚úÖ **Was ge√§ndert werden musste ‚Äì Klar dokumentiert**

*(Alle √Ñnderungen sind im Code unten direkt mit `# ‚Üê ge√§ndert` markiert!)*

---

### üîß Finaler, funktionierender und kommentierter Code (mit Hervorhebung der √Ñnderungen):

```python
import PyPDF2
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy

# ----------------------------------------------------------------------
# NLP-Modell und Stopw√∂rter laden
# ----------------------------------------------------------------------
try:
    nlp = spacy.load("de_core_news_sm")  # ‚Üê ge√§ndert
except OSError:
    print("Fehler: Das spaCy-Modell 'de_core_news_sm' wurde nicht gefunden.")  # ‚Üê ge√§ndert
    print("Stelle sicher, dass du es mit 'python -m spacy download de_core_news_sm' heruntergeladen hast.")  # ‚Üê ge√§ndert
    exit()  # ‚Üê ge√§ndert

stop_words = set(stopwords.words('german'))  # bleibt gleich

# ----------------------------------------------------------------------
# Hilfsfunktion zur Erkennung des Entit√§tstyps in der Frage
# ----------------------------------------------------------------------
def detect_entity_type(question: str) -> str:
    org_keywords = ["organisation", "unternehmen", "firma", "institut", "beh√∂rde", "verein", "gmbh", "kg", "ag"]
    person_keywords = ["person", "name", "wer", "autor", "sprecher", "mitglied", "chef", "leiter", "direktor", "vorsitzender", "ceo"]

    question_lower = question.lower()
    if any(keyword in question_lower for keyword in org_keywords):
        return "ORG"
    elif any(keyword in question_lower for keyword in person_keywords):
        return "PER"
    return None

# ----------------------------------------------------------------------
# Hauptfunktion zur PDF-Verarbeitung
# ----------------------------------------------------------------------
def process_pdf(pdf_path: str) -> tuple[list, list, list, list]:  # ‚Üê ge√§ndert
    print(f"Agent Frontula: PDF-Informationen von '{pdf_path}' empfangen, Verarbeitung gestartet...")
    try:
        with open(pdf_path, 'rb') as pdf_file:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text()
            print("Agent Frontula: Textinhalt aus PDF extrahiert.")

            sentences = sent_tokenize(text, language='german')
            print(f"Agent Frontula: Text in {len(sentences)} S√§tze zerlegt.")

            processed_sentences = []
            for sentence in sentences:
                words = word_tokenize(sentence, language='german')
                words_clean = [word.lower() for word in words if word.lower() not in stop_words and word.isalnum()]
                processed_sentences.append(" ".join(words_clean))
            print("Agent Frontula: Wort-Tokenisierung und Stopwort-Entfernung durchgef√ºhrt.")

            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(processed_sentences)
            word_index_map = vectorizer.vocabulary_
            idf_values = vectorizer.idf_
            index_word_map = {v: k for k, v in word_index_map.items()}
            word_idf_pairs = [(index_word_map[i], idf_values[i]) for i in range(len(idf_values))]
            sorted_word_idf_pairs = sorted(word_idf_pairs, key=lambda x: x[1], reverse=True)
            keywords = sorted_word_idf_pairs[:10]
            print("Agent Frontula: TF-IDF Keyword-Extraktion durchgef√ºhrt.")

            doc = nlp(text)
            entities = [(ent.text, ent.label_) for ent in doc.ents]
            print(f"Agent Frontula: Named Entity Recognition durchgef√ºhrt. {len(entities)} Entit√§ten gefunden.")

            filtered_entities = [ent for ent in entities if ent[1] in ["ORG", "PER"]]
            print(f"Agent Frontula: NER-Ergebnisse nach Typ gefiltert. {len(filtered_entities)} Entit√§ten nach Filterung.")

            entity_counter = Counter(filtered_entities)
            sorted_entities_by_frequency = entity_counter.most_common()
            print("Agent Frontula: Entit√§tenh√§ufigkeit gez√§hlt.")

            return keywords, sorted_entities_by_frequency, sentences, filtered_entities

    except FileNotFoundError:
        print(f"Agent Frontula: Fehler! PDF-Datei '{pdf_path}' nicht gefunden.")
        return [], [], [], []  # ‚Üê ge√§ndert
    except Exception as e:
        print(f"Agent Frontula: Fehler bei der PDF-Verarbeitung oder NLP: {e}")
        return [], [], [], []  # ‚Üê ge√§ndert

# ----------------------------------------------------------------------
# Fragebeantwortung mit Entity-Aware Fallback
# ----------------------------------------------------------------------
def answer_question(question: str, keywords: list, sorted_entities: list, sentences: list, filtered_entities: list) -> str:
    print(f"Agent Frontula: Beantworte Frage: '{question}' (Entity-Aware QA)...")

    question_words = [word.lower() for word in word_tokenize(question, language='german') if word.lower() not in stop_words and word.isalnum()]
    prioritized = []
    fallback = []
    entity_type = detect_entity_type(question)  # ‚Üê ge√§ndert

    if entity_type:
        print(f"Agent Frontula: Frage zielt auf Entit√§tstyp: {entity_type}")
        for sentence in sentences:
            doc = nlp(sentence)
            if any(ent.label_ == entity_type for ent in doc.ents):
                prioritized.append(sentence)

        if prioritized:
            print("Agent Frontula: Antwort basierend auf Entity-Priorisierung gefunden.")
            return "\n".join(prioritized)

    print("Agent Frontula: Fallback zu Keyword-basierter Suche...")
    for sentence in sentences:
        tokens = [word.lower() for word in word_tokenize(sentence, language='german')]
        if any(qw in tokens for qw in question_words):
            fallback.append(sentence)

    if fallback:
        print("Agent Frontula: Antwort basierend auf Keyword-Suche gefunden.")
        return "\n".join(fallback)
    else:
        return "Agent Frontula: Keine passende Antwort im Dokument gefunden."

# ----------------------------------------------------------------------
# Dummy-Warteschleife
# ----------------------------------------------------------------------
def wait_for_next_task():
    print("Agent Frontula: Warte auf die n√§chste Aufgabe und Anweisungen...")

# ----------------------------------------------------------------------
# Hauptprogramm
# ----------------------------------------------------------------------
if __name__ == "__main__":
    pdf_path = input("Bitte gib den Pfad zur PDF-Datei ein: ")  # ‚Üê ge√§ndert
    pdf_keywords, sorted_pdf_entities, pdf_sentences, filtered_pdf_entities = process_pdf(pdf_path)

    if pdf_keywords or sorted_pdf_entities or pdf_sentences or filtered_pdf_entities:  # ‚Üê ge√§ndert
        print("\nExtrahierte Keywords (TF-IDF-basiert):")
        for word, tfidf in pdf_keywords:
            print(f"- {word}, TF-IDF: {tfidf:.4f}")

        print("\nGefilterte Entit√§ten (ORG/PER):")
        for (entity, label), freq in sorted_pdf_entities[:10]:
            print(f"- {entity} ({label}) ‚Äì {freq}√ó")

        questions = [
            "Welche Organisationen werden in dem Dokument erw√§hnt?",  # ‚Üê ge√§ndert
            "Wer sind die Autoren in diesem Dokument?",  # ‚Üê ge√§ndert
            "Was sind die wichtigsten Ergebnisse?",  # ‚Üê ge√§ndert
        ]
        for q in questions:
            a = answer_question(q, pdf_keywords, sorted_pdf_entities, pdf_sentences, filtered_pdf_entities)
            print(f"\nFrage: {q}\nAntwort:\n{a}")

        wait_for_next_task()
    else:
        print("Agent Frontula: Keine Daten zum Verarbeiten.")

    print("Agent Frontula: Code-Block abgeschlossen. √úbergabe an n√§chsten Agenten.")
```


..... und nach start des Programmes und der im Projekt befindlichen pdf Datei als eingabe kam folgendes Ergebnis raus:

```

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.178.62:8501

Bitte gib den Pfad zur PDF-Datei ein: C:\Users\ralfk\Documents\Streamlit_CodeAnalyzer_Vollst√§ndig_Mit_Agenten_und_Code.pdf
Agent Frontula: PDF-Informationen von 'C:\Users\ralfk\Documents\Streamlit_CodeAnalyzer_Vollst√§ndig_Mit_Agenten_und_Code.pdf' empfangen, Verarbeitung gestartet...
Agent Frontula: Textinhalt aus PDF extrahiert.
Agent Frontula: Text in 14 S√§tze zerlegt.
Agent Frontula: Wort-Tokenisierung und Stopwort-Entfernung durchgef√ºhrt.
Agent Frontula: TF-IDF Keyword-Extraktion durchgef√ºhrt.
Agent Frontula: Named Entity Recognition durchgef√ºhrt. 108 Entit√§ten gefunden.
Agent Frontula: NER-Ergebnisse nach Typ gefiltert. 42 Entit√§ten nach Filterung.
Agent Frontula: Entit√§tenh√§ufigkeit gez√§hlt.

Extrahierte Keywords (TF-IDF-basiert):
- 10, TF-IDF: 3.0149
- agenten, TF-IDF: 3.0149
- alias, TF-IDF: 3.0149
- analyseausgabe, TF-IDF: 3.0149
- analysieren, TF-IDF: 3.0149
- analysierte, TF-IDF: 3.0149
- angepasst, TF-IDF: 3.0149
- angepasste, TF-IDF: 3.0149
- anweisung, TF-IDF: 3.0149
- anwendungsbeispiel, TF-IDF: 3.0149

Gefilterte Entit√§ten (ORG/PER):
- as f (ORG) ‚Äì 5√ó
- core (ORG) ‚Äì 3√ó
- ):
     (PER) ‚Äì 3√ó
- Sourcix (PER) ‚Äì 2√ó
- FunctionDef (ORG) ‚Äì 2√ó
- functions (ORG) ‚Äì 2√ó
- classes (ORG) ‚Äì 2√ó
- analysis['file']}\n (PER) ‚Äì 2√ó
- analysis['num_functions']}\n"
     (PER) ‚Äì 2√ó
- analysis['num_classes']}\n (PER) ‚Äì 2√ó
Agent Frontula: Beantworte Frage: 'Welche Organisationen werden in dem Dokument erw√§hnt?' (Entity-Aware QA)...
Agent Frontula: Frage zielt auf Entit√§tstyp: ORG
Agent Frontula: Antwort basierend auf Entity-Priorisierung gefunden.

Frage: Welche Organisationen werden in dem Dokument erw√§hnt?
Antwort:
Verantwortlichkeiten der Agenten
Sourcix ‚Äì Modulverfolgung: Findet alle Importe im Hauptcode und l√∂st lokale Datei-Importe rekursiv auf
Syntaxius ‚Äì AST-Parser: Nutzt ast, um den Code syntaktisch zu analysieren
Efficio ‚Äì Bewertungslogik: Bewertet Codequalit√§t mit einfachen Metriken
Explainix ‚Äì Berichtsgenerator: Erstellt menschenlesbare Berichte
Frontula ‚Äì Streamlit-Entwicklung: Erstellt die Benutzeroberfl√§che
Gemini-AI ‚Äì Sprach-KI: Interpretiert und bewertet Code semantisch
Funktionsbeschreibung
Benutzeroberfl√§che: Streamlit-basierte GUI zur Auswahl des Projektverzeichnisses und Anzeige der Analyse
Importerkennung: Rekursive Erkennung aller Importanweisungen, inkl. lokaler Dateien
Statische Analyse mit AST: Funktionen, Klassen, Kontrollstrukturen, Komplexit√§tsanalyse
KI-gest√ºtzte Bewertung: Gemini AI erstellt Bewertungen, Hinweise und Verbesserungsvorschl√§ge
Technische Vorgaben
Python-Version: 3.12
GUI: Streamlit
Module: os, ast, pathlib, importlib.util, streamlit, optional: radon, bandit, textwrap, difflib
Anwendungsbeispiel
Projektstruktur:
projekt/
‚ñ†‚ñ†‚ñ† main.py (enth√§lt from b1 import o)
‚ñ†‚ñ†‚ñ† b1/
    ‚ñ†‚ñ†‚ñ† o.py (wird rekursiv gefunden)
Analyseausgabe:
- Analysierte Dateien: main.py, b1/o.py
- main.py: 2 Funktionen, 1 Klasse, 3 Schw√§chen erkannt
- b1/o.py: Funktion 'calculate()' zu komplex
- Gesamtscore: 7.2 / 10
Besondere Hinweise
Gemini AI nur f√ºr semantische Bewertungen einsetzen
Importverfolgung durch Sourcix lokal
Klarheit, Modularit√§t und Erweiterbarkeit sind essenziell
Gew√ºnschtes Ergebnis
Struktur:
- app.py (Streamlit-GUI)
- core/import_resolver.py (Import-Scanner)
- core/analyzer.py (AST-Analyse)
- core/reporter.py (KI-Bericht mit Gemini)
- Beispielprojekt zum Testen
Streamlit CodeAnalyzer ‚Äì Vollst√§ndige Dokumentation
Projektbeschreibung, Code, Erweiterungen und AI-Integration (Gemini)
Projektdateien
app.py ‚Äì Streamlit-Oberfl√§che
import streamlit as st
from core.import_resolver import resolve_imports
from core.analyzer import analyze_code
from core.reporter import generate_report
st.title("Streamlit CodeAnalyzer")
st.write("Analyse von Python-Dateien inkl. rekursiver Import-Aufl√∂sung.")
uploaded_file = st.file_uploader("W√§hle eine Hauptdatei (main.py)", type="py")
if uploaded_file:
    filepath = f"/tmp/{uploaded_file.name}"
    with open(filepath, "wb") as f:
        f.write(uploaded_file.getvalue())

    files_to_analyze = resolve_imports(filepath)
    all_results = []

    for file in files_to_analyze:
        analysis = analyze_code(file)
        report = generate_report(analysis)
        all_results.append((file, report))

    for file, report in all_results:
        st.subheader(f"Bericht: {file}")
        st.text(report)
core/import_resolver.py ‚Äì Importerkennung
import os
import ast
def resolve_imports(file_path, project_root=None, visited=None):
    if visited is None:
        visited = set()
    if project_root is None:
        project_root = os.path.dirname(file_path)

    if file_path in visited:
        return []

    visited.add(file_path)
    files = [file_path]

    with open(file_path, "r", encoding="utf-8") as f:
        node = ast.parse(f.read(), filename=file_path)

    for n in ast.walk(node):
        if isinstance(n, ast.Import):
            for alias in n.names:
                module_path = alias.name.replace(".
", "/") + ".py"
                full_path = os.path.join(project_root, module_path)
                if os.path.exists(full_path):
                    files.extend(resolve_imports(full_path, project_root, visited))
    return files
core/analyzer.py ‚Äì AST-Codeanalyse
import ast
def analyze_code(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read(), filename=file_path)
    functions = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]

    return {
        "file": file_path,
        "functions": functions,
        "classes": classes,
        "num_functions": len(functions),
        "num_classes": len(classes)
    }
core/reporter.py ‚Äì Berichtsgenerierung
def generate_report(analysis):
    report = f"Datei: {analysis['file']}\n"
    report += f"Funktionen: {analysis['num_functions']}\n"
    report += f"Klassen: {analysis['num_classes']}\n"
    return report
example/main.py ‚Äì Beispiel Hauptdatei
from b1 import o
def main():
    print("Hauptprogramm l√§uft.")
o.zeige()
if __name__ == "__main__":
    main()
example/b1/o.py ‚Äì Beispiel Modul
def zeige():
    print("Modul b1.o wurde importiert.")
core/reporter.py
# Platzhalter f√ºr die Gemini AI Integration
def get_gemini_analysis(code):
    """
    Sendet Code zur Analyse an Gemini AI und erh√§lt eine Bewertung.
    """
]
    }
def generate_report(analysis):
    report = f"Datei: {analysis['file']}\n"
    report += f"Funktionen: {analysis['num_functions']}\n"
    report += f"Klassen: {analysis['num_classes']}\n"
    # Gemini AI Analyse einholen
    gemini_analysis = get_gemini_analysis(analysis['code'])  # √úbergabe des Codes an Gemini AI
    report += f"\nKI-Bewertung (Gemini AI):\n"
    report += f"- Gesamtscore: {gemini_analysis['score']}\n"
    report += f"- Verbesserungsvorschl√§ge:\n"
    for suggestion in gemini_analysis['suggestions']:
        report += f"  - {suggestion}\n"
    return report
core/analyzer.py
import ast
import radon.complexity as complexity  # Beispiel f√ºr optionale Modulintegration
def analyze_code(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read(), filename=file_path)
        code = f.read() # Den gesamten Code lesen
    functions = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
    # Beispiel f√ºr Radon-Integration (Komplexit√§tsanalyse)
    with open(file_path, 'r') as f:
        complexities = complexity.cc_rank(f.read())
    return {
        "file": file_path,
        "functions": functions,
        "classes": classes,
        "num_functions": len(functions),
        "num_classes": len(classes),
        "code": code, # √úbergabe des Codes an den Reporter
        "complexities": complexities # √úbergabe der Komplexit√§tswerte
    }
Agent Frontula: Beantworte Frage: 'Wer sind die Autoren in diesem Dokument?' (Entity-Aware QA)...
Agent Frontula: Frage zielt auf Entit√§tstyp: PER
Agent Frontula: Antwort basierend auf Entity-Priorisierung gefunden.

Frage: Wer sind die Autoren in diesem Dokument?
Antwort:
Verantwortlichkeiten der Agenten
Sourcix ‚Äì Modulverfolgung: Findet alle Importe im Hauptcode und l√∂st lokale Datei-Importe rekursiv auf
Syntaxius ‚Äì AST-Parser: Nutzt ast, um den Code syntaktisch zu analysieren
Efficio ‚Äì Bewertungslogik: Bewertet Codequalit√§t mit einfachen Metriken
Explainix ‚Äì Berichtsgenerator: Erstellt menschenlesbare Berichte
Frontula ‚Äì Streamlit-Entwicklung: Erstellt die Benutzeroberfl√§che
Gemini-AI ‚Äì Sprach-KI: Interpretiert und bewertet Code semantisch
Funktionsbeschreibung
Benutzeroberfl√§che: Streamlit-basierte GUI zur Auswahl des Projektverzeichnisses und Anzeige der Analyse
Importerkennung: Rekursive Erkennung aller Importanweisungen, inkl. lokaler Dateien
Statische Analyse mit AST: Funktionen, Klassen, Kontrollstrukturen, Komplexit√§tsanalyse
KI-gest√ºtzte Bewertung: Gemini AI erstellt Bewertungen, Hinweise und Verbesserungsvorschl√§ge
Technische Vorgaben
Python-Version: 3.12
GUI: Streamlit
Module: os, ast, pathlib, importlib.util, streamlit, optional: radon, bandit, textwrap, difflib
Anwendungsbeispiel
Projektstruktur:
projekt/
‚ñ†‚ñ†‚ñ† main.py (enth√§lt from b1 import o)
‚ñ†‚ñ†‚ñ† b1/
    ‚ñ†‚ñ†‚ñ† o.py (wird rekursiv gefunden)
Analyseausgabe:
- Analysierte Dateien: main.py, b1/o.py
- main.py: 2 Funktionen, 1 Klasse, 3 Schw√§chen erkannt
- b1/o.py: Funktion 'calculate()' zu komplex
- Gesamtscore: 7.2 / 10
Besondere Hinweise
Gemini AI nur f√ºr semantische Bewertungen einsetzen
Importverfolgung durch Sourcix lokal
Klarheit, Modularit√§t und Erweiterbarkeit sind essenziell
Gew√ºnschtes Ergebnis
Struktur:
- app.py (Streamlit-GUI)
- core/import_resolver.py (Import-Scanner)
- core/analyzer.py (AST-Analyse)
- core/reporter.py (KI-Bericht mit Gemini)
- Beispielprojekt zum Testen
Streamlit CodeAnalyzer ‚Äì Vollst√§ndige Dokumentation
Projektbeschreibung, Code, Erweiterungen und AI-Integration (Gemini)
Projektdateien
app.py ‚Äì Streamlit-Oberfl√§che
import streamlit as st
from core.import_resolver import resolve_imports
from core.analyzer import analyze_code
from core.reporter import generate_report
st.title("Streamlit CodeAnalyzer")
st.write("Analyse von Python-Dateien inkl. rekursiver Import-Aufl√∂sung.")
uploaded_file = st.file_uploader("W√§hle eine Hauptdatei (main.py)", type="py")
if uploaded_file:
    filepath = f"/tmp/{uploaded_file.name}"
    with open(filepath, "wb") as f:
        f.write(uploaded_file.getvalue())

    files_to_analyze = resolve_imports(filepath)
    all_results = []

    for file in files_to_analyze:
        analysis = analyze_code(file)
        report = generate_report(analysis)
        all_results.append((file, report))

    for file, report in all_results:
        st.subheader(f"Bericht: {file}")
        st.text(report)
core/import_resolver.py ‚Äì Importerkennung
import os
import ast
def resolve_imports(file_path, project_root=None, visited=None):
    if visited is None:
        visited = set()
    if project_root is None:
        project_root = os.path.dirname(file_path)

    if file_path in visited:
        return []

    visited.add(file_path)
    files = [file_path]

    with open(file_path, "r", encoding="utf-8") as f:
        node = ast.parse(f.read(), filename=file_path)

    for n in ast.walk(node):
        if isinstance(n, ast.Import):
            for alias in n.names:
                module_path = alias.name.replace(".
", "/") + ".py"
                full_path = os.path.join(project_root, module_path)
                if os.path.exists(full_path):
                    files.extend(resolve_imports(full_path, project_root, visited))
    return files
core/analyzer.py ‚Äì AST-Codeanalyse
import ast
def analyze_code(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read(), filename=file_path)
    functions = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]

    return {
        "file": file_path,
        "functions": functions,
        "classes": classes,
        "num_functions": len(functions),
        "num_classes": len(classes)
    }
core/reporter.py ‚Äì Berichtsgenerierung
def generate_report(analysis):
    report = f"Datei: {analysis['file']}\n"
    report += f"Funktionen: {analysis['num_functions']}\n"
    report += f"Klassen: {analysis['num_classes']}\n"
    return report
example/main.py ‚Äì Beispiel Hauptdatei
from b1 import o
def main():
    print("Hauptprogramm l√§uft.")
Angepasste Dateien im √úberblick ‚Äì Gemini AI Integration
Die Integration von Gemini AI und optionalen Modulen erfordert √Ñnderungen in den folgenden Dateien:
* core/reporter.py
* core/analyzer.py
Die Datei core/reporter.py wird so angepasst, dass sie einen Platzhalter f√ºr die Gemini AI-Integration
enth√§lt.
In core/analyzer.py werden optionale Module wie radon f√ºr Komplexit√§tsanalysen integriert, um
zus√§tzliche
Metriken und Informationen f√ºr Gemini AI zu liefern.
]
    }
def generate_report(analysis):
    report = f"Datei: {analysis['file']}\n"
    report += f"Funktionen: {analysis['num_functions']}\n"
    report += f"Klassen: {analysis['num_classes']}\n"
    # Gemini AI Analyse einholen
    gemini_analysis = get_gemini_analysis(analysis['code'])  # √úbergabe des Codes an Gemini AI
    report += f"\nKI-Bewertung (Gemini AI):\n"
    report += f"- Gesamtscore: {gemini_analysis['score']}\n"
    report += f"- Verbesserungsvorschl√§ge:\n"
    for suggestion in gemini_analysis['suggestions']:
        report += f"  - {suggestion}\n"
    return report
core/analyzer.py
import ast
import radon.complexity as complexity  # Beispiel f√ºr optionale Modulintegration
def analyze_code(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        tree = ast.parse(f.read(), filename=file_path)
        code = f.read() # Den gesamten Code lesen
    functions = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
    classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
    # Beispiel f√ºr Radon-Integration (Komplexit√§tsanalyse)
    with open(file_path, 'r') as f:
        complexities = complexity.cc_rank(f.read())
    return {
        "file": file_path,
        "functions": functions,
        "classes": classes,
        "num_functions": len(functions),
        "num_classes": len(classes),
        "code": code, # √úbergabe des Codes an den Reporter
        "complexities": complexities # √úbergabe der Komplexit√§tswerte
    }
Agent Frontula: Beantworte Frage: 'Was sind die wichtigsten Ergebnisse?' (Entity-Aware QA)...
Agent Frontula: Fallback zu Keyword-basierter Suche...

Frage: Was sind die wichtigsten Ergebnisse?
Antwort:
Agent Frontula: Keine passende Antwort im Dokument gefunden.
Agent Frontula: Warte auf die n√§chste Aufgabe und Anweisungen...
Agent Frontula: Code-Block abgeschlossen. √úbergabe an n√§chsten Agenten.
```
